import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import re
import tkinter as tk
from tkinter import filedialog
import bisect

def extract_capacity(folder_path):
    """
    Extract the capacity (mAh) from the top level folder path.
    
    Args:
        folder_path (str): Path to the top level folder
        
    Returns:
        int: Capacity value in mAh
    """
    # Get the top level folder name
    top_folder = os.path.basename(os.path.normpath(folder_path))
    
    # Use regex to find the number before "mAh"
    match = re.search(r'(\d+)mAh', top_folder)
    if match:
        return int(match.group(1))
    else:
        raise ValueError(f"Could not extract capacity from folder name: {top_folder}")


def set_pne_paths():
    """
    Set up paths for PNE data processing using a file dialog.
    
    Returns:
        tuple: Tuple containing cyclename, cyclepath, and mincapacity lists
    """
    # Initialize tkinter
    root = tk.Tk()
    root.withdraw()  # Hide the main window
    
    # Ask user to select the TXT file
    try:
        # Try to use D:/Work_pc_D/datapath directory first
        datafilepath = filedialog.askopenfilename(initialdir=r"D:/Work_pc_D/datapath", title="Choose Test files", filetypes=[("Text files", "*.txt")])
    except:
        # Fall back to home directory if the specified directory is not accessible
        datafilepath = filedialog.askopenfilename(initialdir=os.path.expanduser("~"), title="Choose Test files", filetypes=[("Text files", "*.txt")])
    
    if not datafilepath:
        raise ValueError("No file selected")
    
    # Read the CSV file with tab separator
    df = pd.read_csv(datafilepath, sep="\t", engine="c", encoding="UTF-8", 
                    skiprows=1, on_bad_lines='skip')
    
    # Extract cyclename and cyclepath
    cyclename = df['cyclename'].tolist() if 'cyclename' in df.columns else []
    cyclepath = df['cyclepath'].tolist() if 'cyclepath' in df.columns else []
    
    # Extract capacity from each cyclepath to create mincapacity list
    mincapacity = []
    for path in cyclepath:
        match = re.search(r'(\d+)mAh', path)
        if match:
            capacity = int(match.group(1))
            mincapacity.append(capacity)
        else:
            mincapacity.append(0)  # Default value if capacity not found
    
    return cyclename, cyclepath, mincapacity

def pne_search_cycle(rawdir, inicycle=None, endcycle=None):
    """
    Search for cycle files in the specified directory within the given cycle range.
    
    Args:
        rawdir (str): Directory containing the cycle files
        inicycle (int, optional): Initial cycle number to start from
        endcycle (int, optional): End cycle number to stop at
        
    Returns:
        list: List of file paths that match the cycle range criteria
    """
    if not os.path.isdir(rawdir):
        return []
    
    # Get all CSV files in the directory
    subfiles = [f for f in os.listdir(rawdir) if f.endswith(".csv")]
    
    # Directly find SaveEndData file
    save_end_data_file = next((f for f in subfiles if "SaveEndData" in f), None)
    
    if save_end_data_file:
        df = pd.read_csv(os.path.join(rawdir, save_end_data_file), sep=",", skiprows=0, engine="c", header = None, encoding="cp949", on_bad_lines='skip')
        if inicycle is not None:
            # #27 is Total cycle number
            index_min = df.loc[(df.loc[:,27]==(inicycle-1)),0].tolist()
        else:
            index_min = df.loc[(df.loc[:,27]==(1)),0].tolist()
            
        if endcycle is not None:
            index_max = df.loc[(df.loc[:,27]==endcycle),0].tolist()
        else:
            index_max = df.loc[(df.loc[:,27]==df.loc[:,27].max()),0].tolist()
            
        df2 = pd.read_csv(os.path.join(rawdir, "savingFileIndex_start.csv"), sep="\\s+", skiprows=0, engine="c", header = None, encoding="cp949", on_bad_lines='skip')
        df2 = df2.loc[:,3].tolist() #result index number
        
        index2 = []
        for element in df2:
            new_element = int(element.replace(',',''))
            index2.append(new_element)
        if len(index_min) != 0:
            file_start = bisect.bisect_left(index2, index_min[-1]+1)-1
            file_end = bisect.bisect_left(index2, index_max[-1]+1)-1
        else:
            file_start = -1
            file_end = -1

    return file_start, file_end

            

def pne_continue_data(path, inicycle=None, endcycle=None):
    
    df = pd.DataFrame()
    profile_raw = None
    
    # Check for Restore directory
    restore_dir = os.path.join(path, "Restore")
    if os.path.isdir(restore_dir):
        print(f"Processing Restore directory at: {restore_dir}")
        
        # Get files within the cycle range
      
        filepos = pne_search_cycle(restore_dir, inicycle, endcycle)
        subfile = [f for f in os.listdir(restore_dir) if f.endswith(".csv")]
        
        if filepos[0] != -1:
            for files in subfile[(filepos[0]):(filepos[1]+1)]:
                if "SaveData" in files:
                    profileRawTemp = pd.read_csv(os.path.join(restore_dir, files), sep=",", skiprows=0, engine="c", header = None, encoding="cp949", on_bad_lines='skip')
                    if profile_raw is not None:
                        profile_raw = pd.concat([profile_raw, profileRawTemp], ignore_index=True)
                    else:
                        profile_raw = profileRawTemp
    
    # Store the profile data in the DataFrame properly
    if profile_raw is not None:
        df = profile_raw
        
    return df        

def pne_cyc_continue(path):
    df = pd.DataFrame()
    restore_dir = os.path.join(path, "Restore")
    if os.path.isdir(restore_dir):
        print(f"Processing Restore directory at: {restore_dir}")
        subfile = [f for f in os.listdir(restore_dir) if f.endswith(".csv")]
  
        save_end_data_file = next((f for f in subfile if "SaveEndData" in f), None)
        if save_end_data_file:
                df.Cycrawtemp = pd.read_csv(os.path.join(restore_dir, save_end_data_file), sep=",", skiprows=0, engine="c", header = None, encoding="cp949", on_bad_lines='skip')
    return df     


def extract_channel_number(path):
    """
    Extract channel number from a path string.
    
    Args:
        path (str): Path containing channel information
        
    Returns:
        str: Extracted channel number or None if not found
    """
    # Look for patterns like "M01Ch045[045]" or "M01Ch046[046]"
    match = re.search(r'M\d+Ch(\d+)(?:\[(\d+)\])?', path)
    if match:
        # Return both the channel number and the number in brackets if available
        channel = match.group(1)
        channel_id = match.group(2) if match.group(2) else channel
        return channel, channel_id
    return None, None


def concatenate():
    cyclename, cyclepath, mincapacity = set_pne_paths()
    
    # Check if paths were successfully loaded
    if not cyclepath:
        raise ValueError("No cycle paths found")
    
    # Ask for cycle range (or use default values)
    inicycle = None
    endcycle = None
    try:
        user_input = input("Enter initial cycle number (or press Enter for all cycles): ").strip()
        if user_input:
            inicycle = int(user_input)
        
        user_input = input("Enter end cycle number (or press Enter for all cycles): ").strip()
        if user_input:
            endcycle = int(user_input)
    except ValueError:
        print("Invalid cycle numbers provided. Using all cycles.")
    
    print(f"Processing cycles: {'all' if inicycle is None else inicycle} to {'all' if endcycle is None else endcycle}")
    
    # Organize paths by cyclename
    organized_data = {}
    
    # First, group all paths by cyclename
    for i, path in enumerate(cyclepath):
        cycname = cyclename[i]
        if cycname not in organized_data:
            organized_data[cycname] = {'paths': [], 'indices': []}
        
        organized_data[cycname]['paths'].append(path)
        organized_data[cycname]['indices'].append(i)
    
    # For each cyclename, collect all subfolders
    for cycname, data in organized_data.items():
        all_subfolders = []
        for path in data['paths']:
            subfolders = [f.path for f in os.scandir(path) if f.is_dir() and "Pattern" not in f.path]
            all_subfolders.append(subfolders)
        
        # Store subfolders in the organized_data dictionary
        data['all_subfolders'] = all_subfolders
    
    # Dictionary to store all channel-specific dataframes
    output_data = {}
    
    # Process each cyclename group
    for cycname, data in organized_data.items():
        all_subfolders = data['all_subfolders']
        
        # Debug print
        print(f"\nProcessing cyclename: {cycname}")
        for i, subfolders in enumerate(all_subfolders):
            print(f"Cycle[{i}] has {len(subfolders)} subfolders:")
            for j, subfolder in enumerate(subfolders):
                channel, channel_id = extract_channel_number(subfolder)
                print(f"  all_subfolders[{i}][{j}]\t{subfolder}\tcycle[{i}]\t{cycname}\tchannel: {channel}, id: {channel_id}")
        
        # Process each cycle
        for cycle_idx, subfolders in enumerate(all_subfolders):
            # Process each subfolder for this cycle
            for subfolder in subfolders:
                channel, channel_id = extract_channel_number(subfolder)
                if channel:
                    # Create a unique key for each channel in each subfolder
                    # Include both channel number and cycle info in the key
                    channel_key = f"{cycname}_Ch{channel_id}"
                    
                    # Extract data from the subfolder
                    pneProfile = pne_continue_data(subfolder, inicycle, endcycle)
                    
                    if not pneProfile.empty:
                        if channel_key not in output_data:
                            output_data[channel_key] = []
                        
                        # Store the profile data with cycle information
                        output_data[channel_key].append({
                            'cycle_idx': cycle_idx,
                            'profile': pneProfile,
                            'subfolder': subfolder
                        })
                        print(f"Added data for {channel_key}, cycle {cycle_idx}, subfolder: {os.path.basename(subfolder)}")
    
    # Now merge the data for each unique channel key
    merged_data = {}
    for channel_key, profiles_list in output_data.items():
        # Sort by cycle_idx to ensure proper order
        profiles_list.sort(key=lambda x: x['cycle_idx'])
        
        if len(profiles_list) > 0:
            # Merge all profiles for this channel
            merged_profile = pd.concat([item['profile'] for item in profiles_list], ignore_index=True)
            
            # Add new column: Calculate total time in seconds with accumulation
            # Column 18 is time(day), Column 19 is time(1/100s)
            # Need to accumulate time values as they reset to 0 in each concatenated dataset
            time_col = merged_profile.shape[1]
            merged_profile[time_col] = 0  # Initialize the new time column
            
            # Track accumulated time
            accumulated_days = 0
            accumulated_time = 0
            last_day = 0
            last_time = 0
            
            # Process each profile separately to handle resets
            start_idx = 0
            for item in profiles_list:
                profile = item['profile']
                end_idx = start_idx + len(profile)
                
                # If not the first profile, check for time reset
                if start_idx > 0:
                    current_day = merged_profile.iloc[start_idx][merged_profile.columns[18]]
                    current_time = merged_profile.iloc[start_idx][merged_profile.columns[19]]
                    
                    # If time resets (current time is less than previous time), accumulate the previous time
                    if current_day < last_day or (current_day == last_day and current_time < last_time):
                        accumulated_days += last_day
                        accumulated_time += last_time
                
                # Calculate time for this segment with accumulation
                days = merged_profile.iloc[start_idx:end_idx][merged_profile.columns[18]]
                times = merged_profile.iloc[start_idx:end_idx][merged_profile.columns[19]]
                
                # Update the time column with accumulated values
                merged_profile.iloc[start_idx:end_idx, time_col] = ((accumulated_days + days) * 8640000 + 
                                                                    (accumulated_time + times)) / 100
                
                # Update for next iteration
                last_day = merged_profile.iloc[end_idx-1][merged_profile.columns[18]]
                last_time = merged_profile.iloc[end_idx-1][merged_profile.columns[19]]
                start_idx = end_idx
            
            merged_data[channel_key] = merged_profile
            # Export merged profile to CSV
            output_filename = f"{channel_key}_merged_profile.csv"
            merged_profile.to_csv(output_filename, index=False)
            print(f"Exported merged profile data for {channel_key} to {output_filename}")
            
            # Plot column 48 (time in seconds) vs column 9 (voltage or current data)
            # Create an interactive plot similar to MATLAB
            plt.figure(figsize=(12, 6))
            fig, ax = plt.subplots(figsize=(12, 6))
            line, = ax.plot(merged_profile[time_col], merged_profile[9], 'b-')
            ax.set_title(f"{channel_key} - Time vs Data")
            ax.set_xlabel("Time (seconds)")
            ax.set_ylabel("Value (Column 9)")
            ax.grid(True)
            
            # Add interactive features
            from matplotlib.widgets import Slider, Button, RectangleSelector
            
            # Add zoom and pan functionality
            def on_select(eclick, erelease):
                x1, y1 = eclick.xdata, eclick.ydata
                x2, y2 = erelease.xdata, erelease.ydata
                if x1 != x2 and y1 != y2:  # Only zoom if a region is selected
                    ax.set_xlim(min(x1, x2), max(x1, x2))
                    ax.set_ylim(min(y1, y2), max(y1, y2))
                    fig.canvas.draw_idle()
            
            # Add reset button
            plt.subplots_adjust(bottom=0.2)
            reset_ax = plt.axes([0.8, 0.05, 0.1, 0.04])
            reset_button = Button(reset_ax, 'Reset View')
            
            def reset_view(event):
                ax.set_xlim(merged_profile[time_col].min(), merged_profile[time_col].max())
                ax.set_ylim(merged_profile[9].min(), merged_profile[9].max())
                fig.canvas.draw_idle()
            
            reset_button.on_clicked(reset_view)
            
            # Add rectangle selector for zoom
            rect_selector = RectangleSelector(ax, on_select, useblit=True,
                                             button=[1], 
                                             minspanx=5, minspany=5,
                                             spancoords='pixels',
                                             interactive=True)
            
            plt.tight_layout()
            # Save the plot
            plot_filename = f"{channel_key}_plot.png"
            plt.savefig(plot_filename)
            plt.close()
            print(f"Generated plot for {channel_key} and saved to {plot_filename}")
            
            # Print detailed information about what was merged
            print(f"Merged {len(profiles_list)} profiles for {channel_key}:")
            for item in profiles_list:
                print(f"  - Cycle {item['cycle_idx']}: {os.path.basename(item['subfolder'])}")
    
    # Check if any data was processed
    if not merged_data:
        print("Warning: No data was processed. Check your input paths and cycle numbers.")
    else:
        # Print a summary of the processed data
        print("\nData summary:")
        for key, df in merged_data.items():
            print(f"  - {key}: {df.shape[0]} rows, {df.shape[1]} columns")
    
    print(f"\nProcessed {len(merged_data)} unique channels with data")
    return merged_data

def main():
    
    concatenate()

if __name__ == "__main__":
    main()